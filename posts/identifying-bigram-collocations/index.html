<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    
    <meta name="description" content="Review of identifying collocations via hypothesis testing.">
    
    <title>Identifying Bigram Collocations</title>
    <link rel="alternate" type="application/rss+xml" href="https://proceed-to-decode.com/rss.xml"
        title="Proceed to Decode">
    <link rel="stylesheet" href="/css/default.css?version=3">
    
    
    
    
    
    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
    
</head>

<body>
    <nav class="top-menu">
    <div class="top-menu">
        <ul class="left-menu">
            <li class="menu-item"><a href="/">Home</a></li>
            <li class="menu-item"><a href="/about">About</a></li>
        </ul>
        <ul class="right-menu">
            <li class="menu-item"><a href="https://github.com/kldtz"><svg height="32" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a></li>
            <li class="menu-item"><a href="/rss.xml"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" height="32" viewBox="0 0 128.000000 128.000000" preserveAspectRatio="xMidYMid meet"><title>RSS</title><g transform="translate(0.000000,128.000000) scale(0.100000,-0.100000)" stroke="none"><path d="M575 1274 c-11 -2 -45 -9 -75 -15 -216 -42 -418 -239 -475 -464 -19 -76 -19 -234 0 -310 55 -218 237 -402 455 -461 103 -28 254 -24 357 10 276 90 456 353 440 642 -18 323 -276 581 -599 598 -46 3 -92 3 -103 0z m-70 -290 c219 -52 417 -239 484 -457 11 -37 23 -97 27 -132 l7 -65 -66 0 c-72 0 -77 4 -77 68 0 49 -41 161 -80 222 -43 66 -132 148 -206 187 -63 34 -165 63 -221 63 l-33 0 0 63 c0 35 3 67 6 70 8 8 82 -1 159 -19z m-28 -232 c147 -46 258 -164 298 -317 26 -103 25 -105 -49 -105 -70 0 -67 -3 -81 70 -23 113 -140 220 -260 237 l-45 6 0 63 0 64 39 0 c22 0 65 -8 98 -18z m18 -266 c35 -35 41 -69 18 -108 -48 -82 -163 -51 -163 42 0 80 91 120 145 66z"/></g></svg></a></li>
        </ul>
    </div>
</nav>
    <main>
        <article>
            <header>
                <h1>Identifying Bigram Collocations</h1>
                <div class="date"><span class="date">Sep 27, 2020</span></div>
            </header>
            <p>After two posts on FSTs, this text deals with another piece of old (or <em>classic</em>) technology from computational linguistics. It's my digestion of <a href="#manning-sch%C3%BCtze-1999">Manning and Schütze's (1999)</a> chapter on collocations and some cited/related papers.</p>
<p>By the most simple definition, a collocation is a sequence of words that occur together more often then expected by chance. For a bigram $w_1 w_2$, we can formalize this using conditional probabilities as $P(w_1) P(w_2|w_1) &gt; P(w_1) P(w_2)$, or simply $P(w_2|w_1) &gt; P(w_2)$. We can estimate the probabilties from a corpus by counting unigram and bigram types: </p>
<div>$$
P(w_1) = \frac{C(w_1)}{N}, \quad
P(w_2) = \frac{C(w_2)}{N}, \quad
P(w_2|w_1) = \frac{C(w_1, w_2)}{C(w_1)},
$$</div>
<p>where $C(...)$ is a count from our corpus and $N$ is the number of unigrams/bigrams in the corpus<a id="fn-1"></a><sup><a href="#1">1</a></sup>. However, if we want to use these probabilities to find collocations in a corpus, we face two problems:</p>
<ol>
<li>$P(w_2|w_1) &gt; P(w_2)$ could hold by chance, especially if our corpus is small or if we are looking at infrequent pairs of words.</li>
<li>Language is not random, so most sequences of words occur more often than we would expect by the frequency of their individual words. That is, our definition doesn't really capture what linguists understand under the term <em>collocation</em>.</li>
</ol>
<p>The first problem is a question of statistical significance, so an obvious idea is to apply a statistical test that tells us whether the difference between the conditional probability and the probability of two independent events is significant.  Problem two tells us that significance is not enough, we also need to have a look at effect size. We will use the test statistic to rank bigrams from most likely to least likely collocation and then need to find a threshold for classification (usually this is done empirically by inspecting the ranked bigrams of a corpus and choosing a cutoff).</p>
<h2>Hypothesis Testing</h2>
<p>Before picking a test statistic, we need to think about the distribution we are dealing with. We can view a text as a sequence of <em>N</em> (overlapping) bigrams. For a given bigram $b = (w_1, w_2)$, each bigram in this sequence either equals <em>b</em> (1) or not (0). In other words, we have <em>N</em> binary random variables. Assuming independence between these random variables and identical distribution<a id="fn-2"></a><sup><a href="#2">2</a></sup>, we can model text as a <em>Bernoulli process</em>, a sequence of independent Bernoulli trials. The number of successes (bigram equals <em>b</em>) in these N trials has a binomial distribution. The probability mass function (PMF) for the binomial distribution is defined as</p>
<div>$$
b(k; n, p) = {n \choose k} p^k (1-p)^{n-k}
$$</div>
<p>where $n \in \mathbb{N}$ is the number of trials ($N$ bigrams), $k$ (nonnegative integer less or equal $n$) is the number of successes (bigrams that equal <em>b</em>) and $p$ is the success probability of each Bernoulli trial. We have ${n \choose k}$ ways of selecting $k$ out of $n$ trials, and each way adds a probability of $p^k (1-p)^{n-k}$.</p>
<h3>T-test</h3>
<p>According to the <a href="https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem">De Moivre-Laplace theorem</a>, the normal distribution can be used to approximate the binomial distribution for large enough $n$ and/or $p$ not close to 0 or 1, more specifically, if $np (1-p) &gt; 5$ (<a href="#dunning-1993">Dunning, 1993</a>). Under this precondition, we could use a one-sample t-test to determine whether the population mean (the bigram probability estimated from our text sample, $P(w_1, w_2) = \frac{C(w_1, w_2)}{N}$) is significantly different (or larger if we use a one-sided test) from the mean we would expect given the null hypothesis (the product of the probabilties of the individual words, $H_0: P(w_1, w_2) = P(w_1) P(w_2)$). However, <a href="https://proceed-to-decode.com/posts/vocabulary-of-russian-news/">we know</a> that the distribution of words in natural language follows <em>Zipf's law</em>, that is, a few words occur with very high frequency, while many words occur just once or a few times, even in a large corpus. So in many cases $p$ will be close to zero and $np (1-p) &lt; 1$, prohibiting the use of the t-test, which assumes a normal distribution.</p>
<h3>Likelihood ratios</h3>
<p><a href="#dunning-1993">Dunning (1993)</a> proposed <em>likelihood ratios</em> as more appropriate test for collocations. His method does not require normally distributed data and produces more interpretable numbers than the t-test statistic. The computed statistic is asymptotically $\chi ^2$ distributed, so we can consult the same table as we do for <em>Pearson's chi-squared</em> test to determine confidence levels. Furthermore, the statistic is more appropriate for sparse data than Pearson's $X^2$.</p>
<p>We start by defining the hypotheses we want to test. I copy the definitions from <a href="#manning-sch%C3%BCtze-1999">Manning and Schütze's (1999)</a>.</p>
<p>$$
H_0: P(w_2|w_1) = p = P(w_2|\neg w_1)
$$
$$
H_A: P(w_2|w_1) = p_1 \neq p_2 = P(w_2|\neg w_1)
$$</p>
<p>Note that this does not follow directly from the definition above. The test is two-sided because the null hypothesis is an equality (independence of $w_1$ and $w_2$). If it is rejected, we could have $P(w_2|w_1) &gt; P(w_2|\neg w_1)$ or $P(w_2|w_1) &lt; P(w_2|\neg w_1)$. In the latter case, we wouldn't be dealing with a collocation according to our definition. We can solve this problem by additionally testing for the direction.</p>
<p>Now we compute the logarithm of the likelihood ratios, the ratio between the different likelihoods of obtaining the observed frequencies under the two hypotheses.</p>
<div>$$\begin{align}
\log \lambda &= \log \frac{b(c_{12}, c_1, p) \cdot b(c_2 - c_{12}, N - c_1, p)}{b(c_{12}, c_1, p_1) \cdot b(c_2 - c_{12}, N - c_1, p_2)} \\
             &= \log L(c_{12}, c_1, p) + \log L(c_2 - c_{12}, N - c_1, p)\\ 
             &\quad - \log L(c_{12}, c_1, p_1) - \log L(c_2 - c_{12}, N - c_1, p_2)
\end{align}$$</div>
<p>where $L(k; n, x) = x^k (1-x)^{n-k}$ because the binomial coefficients cancel out. Under the null hypothesis, we compute the likelihood of choosing $C(w_1, w_2) = c_{12}$ out of $C(w_1) = c_1$ items and $C(\neg w_1, w_2) = c_2 - c_{12}$ out of $C(\neg w_1) = N - c_1$ items with the same probability $p = \frac{c_2}{N}$. Under the alternative hypothesis, the two PMFs have different probability parameters: $p_1 = \frac{c_{12}}{c_1}$ and $p_2 = \frac{c_2 - c_{12}}{N - c_{1}}$. It's the quantity $- 2 \log \lambda$ that is asymptotically $\chi ^2$ distributed with one degree of freedom.</p>
<table><thead><tr><th align="right"></th><th align="right">w<sub>2</sub></th><th align="right">¬w<sub>2</sub></th><th align="right">∑</th></tr></thead><tbody>
<tr><td align="right"><strong>w<sub>1</sub></strong></td><td align="right">c<sub>12</sub></td><td align="right"><em>c<sub>1</sub> - c<sub>12</sub></em></td><td align="right">c<sub>1</sub></td></tr>
<tr><td align="right"><strong>¬w<sub>1</sub></strong></td><td align="right">c<sub>2</sub> - c<sub>12</sub></td><td align="right"><em>N - c<sub>1</sub> - c<sub>2</sub> + c<sub>12</sub></em></td><td align="right">N - c<sub>1</sub></td></tr>
<tr><td align="right"><strong>∑</strong></td><td align="right">c<sub>2</sub></td><td align="right"><em>N - c<sub>2</sub></em></td><td align="right">N</td></tr>
</tbody></table>
<h2>Application: Period disambiguation</h2>
<p>Besides the obvious applications in lexicography (identifying proper nouns, extracting typical contexts of words) or phraseology (identifying <a href="https://en.wikipedia.org/wiki/Phraseme">phrasemes</a>), an example of a more creative and very successful application of collocation identification has been proposed by Kiss and Strunk (<a href="#kiss-strunk-2002">2002</a>, <a href="#kiss-strunk-2006">2006</a>). They apply different versions of the log-likelihood ratio method introduced above to disambiguate between periods as sentence boundaries and abbreviations. Bigram collocations of truncated word and period tend to be abbreviations (which, however, could appear at the end of a sentence), collocational ties between tokens surrounding a period are evidence against a sentence boundary, whereas collocations of a potential sentence boundary and its subsequent word are evidence for a sentence boundary (frequent sentence starters). Together with a few empirically determined heuristics, these collocation tests are the backbone of the unsupervised sentence boundary detection system <em>Punkt</em> which comes with Python's <a href="https://www.nltk.org/">Natural Language Toolkit (NLTK)</a>. Of course, Kiss and Strunk's (2006) system cannot compete with modern supervised systems that are trained on huge amounts of annotated (domain-specific) data, but often labeled data for our language and domain of interest is unavailable and in these cases <em>Punkt</em> is still a very good choice, given that what we consider <em>sentences</em> is delimited by periods (see <a href="#sanchez-2019">Sanchez (2019)</a> for a recent counter example).</p>
<hr>
<p><a id="1" href="#fn-1"><sup>1</sup></a> For the sake of simplicity, we assume the same number of unigrams and bigrams <em>N</em>.</p>
<p><a id="2" href="#fn-2"><sup>2</sup></a> This is a naive assumption that obviously cannot be true. All models are wrong, but some are useful, as they say.</p>
<hr>
<p><a id="dunning-1993"></a> Dunning, T. E. (1993). Accurate methods for the statistics of surprise and coincidence. <em>Computational linguistics</em>, 19(1), 61-74.</p>
<p><a id="kiss-strunk-2002"></a>Kiss, T., &amp; Strunk, J. (2002). Viewing sentence boundary detection as collocation identification. In <em>Proceedings of KONVENS</em> (Vol. 2002, pp. 75-82).</p>
<p><a id="kiss-strunk-2006"></a>Kiss, T., &amp; Strunk, J. (2006). Unsupervised multilingual sentence boundary detection. <em>Computational linguistics</em>, 32(4), 485-525.</p>
<p><a id="manning-schütze-1999"></a>Manning, C., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press.</p>
<p><a id="sanchez-2019"></a> Sanchez, G. (2019). Sentence boundary detection in legal text. In <em>Proceedings of the Natural Legal Language Processing Workshop 2019</em> (pp. 31-38).</p>

        </article>
    </main>
    <footer id="site-footer">
    <a id="footer-name" href="/">&copy; 2018-2021<br>Tobias Kolditz</a>
</footer>
    
</body>

</html>
