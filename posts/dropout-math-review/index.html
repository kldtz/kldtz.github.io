<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    
    <meta name="description" content="Stochastic functions and some combinatorics for Baldi and Sadowski (2014).">
    
    <title>The Dropout Learning Algorithm: Math Review</title>
    <link rel="alternate" type="application/rss+xml" href="https://proceed-to-decode.com/rss.xml"
        title="Proceed to Decode">
    <link rel="stylesheet" href="/css/default.css?version=2">
    
    
    
    
    
    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
    
</head>

<body>
    <nav class="top-menu">
    <div class="top-menu">
        <ul class="left-menu">
            <li class="menu-item"><a href="/">Home</a></li>
            <li class="menu-item"><a href="/about">About</a></li>
        </ul>
        <ul class="right-menu">
            <li class="menu-item"><a href="https://github.com/kldtz"><svg height="32" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a></li>
            <li class="menu-item"><a href="/rss.xml"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" height="32" viewBox="0 0 128.000000 128.000000" preserveAspectRatio="xMidYMid meet"><g transform="translate(0.000000,128.000000) scale(0.100000,-0.100000)" stroke="none"><path d="M575 1274 c-11 -2 -45 -9 -75 -15 -216 -42 -418 -239 -475 -464 -19 -76 -19 -234 0 -310 55 -218 237 -402 455 -461 103 -28 254 -24 357 10 276 90 456 353 440 642 -18 323 -276 581 -599 598 -46 3 -92 3 -103 0z m-70 -290 c219 -52 417 -239 484 -457 11 -37 23 -97 27 -132 l7 -65 -66 0 c-72 0 -77 4 -77 68 0 49 -41 161 -80 222 -43 66 -132 148 -206 187 -63 34 -165 63 -221 63 l-33 0 0 63 c0 35 3 67 6 70 8 8 82 -1 159 -19z m-28 -232 c147 -46 258 -164 298 -317 26 -103 25 -105 -49 -105 -70 0 -67 -3 -81 70 -23 113 -140 220 -260 237 l-45 6 0 63 0 64 39 0 c22 0 65 -8 98 -18z m18 -266 c35 -35 41 -69 18 -108 -48 -82 -163 -51 -163 42 0 80 91 120 145 66z"/></g></svg></a></li>
        </ul>
    </div>
</nav>
    <main>
        <article>
            <header>
                <h1>The Dropout Learning Algorithm: Math Review</h1>
                <div class="date"><span class="date">Mar 14, 2020</span></div>
            </header>
            <p>Review of combinatorics and probability theory for Baldi &amp; Sadowski (2014).</p>
<h2>Combinatorics</h2>
<p>We have a set $I$ of $n$ elements. The set of all subsets of $I$, the powerset $\mathcal{P}(I)$, has $2^n$ elements (including the empty subset), which can be seen </p>
<ol>
<li><strong>algebraically</strong>, by grouping all subsets of $I$ by cardinality, and noting that for $0 \leq k \leq n$ we have ${n \choose k}$ subsets of cardinality $k$. The sum of these terms is the left side of the binomial theorem $\sum_{k=0}^n {n \choose k} x^{n-k} y^k = (x + y)^n$ with $x = y = 1$, so $\mathcal{P}(I) = 2^n$.</li>
<li><strong>combinatorially</strong>, simply by observing that we can form a subset of $I$ by deciding for each element, whether to include it in the set or not. Thus, there are $2^n$ ways of forming a subset.</li>
</ol>
<p>How often does each element of $I$ appear in all of the $2^n$ subsets of $I$? Again, consider each group of subsets with equal cardinality: We can form all subsets of $I$ of cardinality $k &gt; 0$ that contain the same element $e$ by choosing element $e$ out of $n$ elements and combining it with each subset of carndinality $k-1$ of the remaining $n-1$ elements. Thus, each element of $I$ appears $\sum_{k=1}^{n} {n-1 \choose k-1} = 2^{n-1}$ times.</p>
<h2>Stochastic functions</h2>
<h3>Expectation</h3>
<p>For a discrete random variable $X$ with $k$ possible outcomes $x_1, x_2, \ldots, x_k$ occuring with probabilities $P(X = x_i) = p_i$, $1 \leq i \leq k$, the expectation (<strong>first moment</strong>) of $X$ is defined as</p>
<div>$$
E[X] = \sum_{i=1}^k x_i p_i
$$</div>
<p>So, in the special case of a Bernoulli random variable with $x_1 = 1$, $x_2 = 0$ and $p_1 = p$, $p_2 = 1-p = q$, we have $E[X] = p$. </p>
<p>From the general definition, we can prove the <strong>linearity of expectation</strong>: (1) The expected value of the sum of two (or any finite number of) random variables equals the sum of the expected values of the individual random variables.</p>
<div class="math-left-align">$$\begin{align}
E[X + Y] &= \sum_x \sum_y \left[ (x + y) \cdot P(X = x, Y = y)\right] \\
         &= \sum_x \sum_y \left[ x \cdot P(X = x, Y = y)\right] + \sum_x \sum_y \left[ y \cdot P(X = x, Y = y)\right] \\
         &= \sum_x x \sum_y P(X = x, Y = y) + \sum_y y \sum_{x} P(X = x, Y = y) \\
         &= \sum_x x \cdot P(X = x) + \sum_y y \cdot P(Y = y) \\
         &= E[X] + E[Y]
\end{align}$$</div>
<p>(2) The expected value scales linearly with a multiplicative constant.</p>
<div class="math-left-align">$$\begin{align}
E[aX] &= \sum_x a x \cdot P(X = x) \\
      &= a \sum_x x \cdot P(X = x) \\
      &= a \cdot E[X] \\
\end{align}$$</div>
<p>By definition, two discrete random variables are independent iff $P(X=x, Y=y) = P(X=x) P(Y=y)$ for all $x, y$. Thus for <strong>independent random variables</strong>, the expectation of the product equals the product of the expectations.</p>
<div class="math-left-align">$$\begin{align}
E[XY] &= \sum_x \sum_y xy \cdot P(X=x, Y=y) \\
      &= \sum_x x P(X=x) \sum_y y P(Y=y) \\
      &= E[X] E[Y] \\
\end{align}$$</div>
<h3>Variance</h3>
<p>Variance (the <strong>second central moment</strong>) is defined as the expected squared deviation of a random variable $X$ from its mean $\mu = E[X]$.</p>
<div>$$\begin{align}
Var(X) &= E \left[ (X - \mu)^2 \right] \\
       &= E \left[ X^2 - 2 X E[X] + E[X]^2 \right] \\ 
       &= E[X^2] - 2 \cdot E[X]^2 + E[X]^2 \\ 
       &= E[X^2] - E[X]^2
\end{align}$$</div>
<p>For the special case of a <strong>Bernoulli random variable</strong> we can simplify as follows:</p>
<div>$$\begin{align}
Var(X) &= E[X^2] - E[X]^2 \\ 
       &= p \cdot 1^2 + q \cdot 0^2 - p^2 \\ 
       &= p - p^2 \\ 
       &= p(1 - p) = pq
\end{align}$$</div>
<p>If its argument is scaled by a constant, variance is <strong>scaled by the square</strong> of that constant. This follows from the definition of variance and linearity of expectation:</p>
<div>$$\begin{align}
Var(aX) &= E \left[ (a X - a \mu)^2 \right] \\ 
       &= E \left[ a^2 (X - \mu)^2 \right] \\ 
       &= a^2 E \left[ (X - \mu)^2 \right] \\ 
       &= a^2 Var(X)
\end{align}$$</div>
<h3>Covariance</h3>
<p>Covariance is defined as the expected value of the product of the squared deviations of two jointly distributed random variables from their means.</p>
<div>$$\begin{align}
Cov(X, Y) &= E\left[ (X - E[X]) (Y - E[Y])  \right] \\ 
          &= E\left[ XY - X E[Y] - E[X]Y + E[X]E[Y]  \right] \\ 
          &= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\ 
          &= E[XY] - E[X]E[Y]
\end{align}$$</div>
<p>Variance is a special case of covariance, where $X = Y$. If $X$ and $Y$ are independent random variables, then $E[XY] = E[X]E[Y]$, that is, $Cov(X, Y) = 0$ (this implication does not hold in the opposite direction in general).</p>
<p>The <strong>variance of the sum</strong> of two random variables equals the sum of their individual variances plus two times their covariance.</p>
<div>$$\begin{align}
Var(X + Y) &= E \left[ ((X+Y) - E[X + Y])^2 \right] \\ 
           &= E \left[ ((X+Y)^2 - 2 (X+Y) E[X + Y] + E[X+Y]^2 \right] \\ 
           &= E [ (X^2 + 2XY + Y^2 \\ 
           &\qquad - 2 (X E[X] + X E[Y] + Y E[X] + Y E[Y]) \\ 
           &\qquad + E[X]^2 + 2 E[X] E[Y] + E[Y]^2 ] \\ 
           &= E[ X^2 - 2 X E[X] + E[X]^2 \\ 
           &\qquad + Y^2 - 2 Y E[Y] + E[Y]^2 \\ 
           &\qquad + 2 ( XY - X E[Y] - Y E[X] + E[X] E[Y])] \\ 
           &= E[ (X - E[X])^2 ] + E[ ( Y - E[Y])^2 ] \\  
           &\qquad + 2 \cdot E [ (X - E[X] ) ( Y - E[Y] )] \\ 
           &= Var(X) + Var(Y) + 2 \cdot Cov(X, Y)
\end{align}$$</div>
<p>Because $Cov(X, Y) = 0$ for independent random variables, variance is linear if $X$ and $Y$ are independent.</p>
<hr />
<p>Baldi, P., &amp; Sadowski, P. (2014). The dropout learning algorithm. Artificial intelligence, 210, 78-122.</p>

        </article>
    </main>
    <footer id="site-footer">
    <a id="footer-name" href="/">&copy; 2018-2021<br>Tobias Kolditz</a>
</footer>
    
</body>

</html>
